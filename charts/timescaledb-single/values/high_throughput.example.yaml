# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

# High Throughput requires a good throughput on the I/O layer, for most cloud providers,
# IOPS are related to disk size, therefore setting up a decent sized WAL volume is benificial
# for getting good troughput.
#
# https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-io-characteristics.html
# https://cloud.google.com/compute/docs/disks/performance
#
persistentVolumes:
  data:
    size: 2Ti
  wal:
    # We need good IOPS (see above), but we also need a significant amount of disk space
    # to ensure we have a few hours of previous WAL on disk so replica's or the archiver
    # can catch up if they have had interruptions.
    size: 750Gi

patroni:
  bootstrap:
    method: restore_or_initdb
    restore_or_initdb:
      # Increasing the wal-segsize from the default 16MB to 256MB has the upside of not having
      # as many files per second, and not as many files in the pg_waldir.
      # An immediate benefactor is the backup to s3: there will be less API calls, and many
      # s3 operations with WAL therefore experience a speedup if the default is increased.
      #
      # https://www.postgresql.org/docs/current/app-initdb.html (search for --wal-segsize)
      command: >
        /etc/timescaledb/scripts/restore_or_initdb.sh
        --encoding=UTF8
        --locale=C.UTF-8
        --wal-segsize=256
    dcs:
      # For High Throughput clusters it is likely that the bottleneck will be the recovery
      # of the WAL on the replica's. If the high throughput is continous, this means
      # the replica's will never catch up.
      # Enabling synchronous_mode will ensure that there is at least 1 synchronous replica,
      # which will ensure that the throughput will be limited to whatever the replica process
      # of the synchronous replica can process.
      #
      # https://patroni.readthedocs.io/en/latest/replication_modes.html#synchronous-mode
      synchronous_mode: true
      master_start_timeout: 0
      postgresql:
        # Using replication slots in high throughput situations has a significant danger:
        # The master may need to retain more WAL than the size of the WAL Volume allows,
        # if a replica is catching up.
        #
        # With backup enabled: while the replica is catching up it will be fetching archives
        # from s3, until s3 is exhausted, after which the replica will switch to streaming
        # from the master. While the replica is not yet streaming (this may take hours),
        # the replication slot reserved for it is stale and will cause WAL to be retained by
        # the master.
        # If we have a backup, we don't need replication slots, as s3 will have the WAL, therefore,
        # if the backup is enabled, we can safely disable the use of replication slots.
        use_slots: false
        parameters:
          # For good performance we want to have a long interval between checkpoints,
          # however for continous high throughput, the replica's will have very limited
          # capacity to catch up after a crash/restart.
          #
          # For example, if the recovery process of the replica requires on average
          # 90% of the CPU, it could recover 1.11 seconds of WAL per second.
          # If after a crash it is delayed by 5 minutes, it will only catch up in:
          #
          #   300/(1.11 - 1) = 2727 seconds (roughly 45 minutes)
          #
          # We therefore want to pick a middle ground between recoverability of instances
          # and performance, and for now come up with 5 minutes.
          checkpoint_timeout: 300s
          temp_file_limit: '200GB'
          # remote_apply will throttle the replica's, see also the comment at patroni.bootstrap.synchronous_mode
          synchronous_commit: remote_apply

# Enabling tuning allows us to use this configuration on many types of instances, therefore we enable it
timescaledbTune:
  enabled: true

backup:
  enabled: true

  # For High Throughput clusters, the archiver will likely not be able to keep up if running synchronously.
  # Therefore, we'll need to enable asynchronous archiving.
  #
  # Not using asynchronous archiving may cause failures far in the future, for example, on a 
  # cluster with a WAL Volume of 500GB, and an archiver lagging 200MB/minute, the primary pod will only
  # fail after 2500 minutes, or almost 2 days.
  pgBackRest:archive-push:
    process-max: 4
    archive-async: "y"

  pgBackRest:archive-get:
    process-max: 4
    archive-async: "y"
    archive-get-queue-max: 2GB
