# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

{{- /*
This ConfigMap contains scripts, like bootstrap scripts for the backup and archive scripts for
archive_command
*/ -}}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ template "timescaledb.fullname" . }}-scripts
  labels:
    app: {{ template "timescaledb.fullname" . }}
    chart: {{ template "timescaledb.chart" . }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
    cluster-name: {{ template "clusterName" . }}
data:
  # If no backup is configured, archive_command would normally fail. A failing archive_command on a cluster
  # is going to cause WAL to be kept around forever, meaning we'll fill up Volumes we have quite quickly.
  #
  # Therefore, if the backup is disabled, we always return exitcode 0 when archiving
  pgbackrest_archive.sh: |
    #!/bin/sh

    log() {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - archive - $1"
    }

    [ -z "$1" ] && log "Usage: $0 <WALFILE or DIRECTORY>" && exit 1

    {{- if .Values.fullWalPrevention.enabled }}
    prevent_full_wal() {
        WAL_STATUS_FILE=${WAL_STATUS_FILE:-{{ include "wal_status_file" . }}}
        WAL_CHECK_INTERVAL=${WAL_CHECK_INTERVAL:-{{ .Values.fullWalPrevention.checkFrequency | default 30 | int }}}

        # Only continue the check if the status file is missing, or if it hasn't been touched in a while
        if find "${WAL_STATUS_FILE}" -not -newermt "-${WAL_CHECK_INTERVAL} seconds" -exec false {} +; then
            exit 0
        fi
        touch "${WAL_STATUS_FILE}"

        PGDATA=${PGDATA:-.}
        READ_ONLY_PERCENT=${READ_ONLY_PERCENT:-{{ .Values.fullWalPrevention.thresholds.readOnlyFreePercent | default 5 | int }}}
        READ_ONLY_MB=${READ_ONLY_MB:-{{ .Values.fullWalPrevention.thresholds.readOnlyFreeMB | default 64 | int }}}
        READ_WRITE_PERCENT=${READ_WRITE_PERCENT:-{{ .Values.fullWalPrevention.thresholds.readWriteFreePercent | default 7 | int }}}
        READ_WRITE_MB=${READ_WRITE_MB:-{{ .Values.fullWalPrevention.thresholds.readWriteFreeMB | default 128 | int }}}
        WAL_STAT=$(stat -L -f --format="%a/%b,%a*%S" "$1")
        PCT_AVAILABLE=$(( 100*${WAL_STAT%,*} ))
        MB_AVAILABLE=$(( ${WAL_STAT#*,}/1024/1024 ))
        if grep -qs "default_transaction_read_only" "${WAL_STATUS_FILE}" "${PGDATA}/postgresql.auto.conf"; then
            TRANSACTION_READ_ONLY=1
        else
            TRANSACTION_READ_ONLY=0
        fi

        export PGAPPNAME=prevent_full_wal
        if [ ${TRANSACTION_READ_ONLY} -eq 0 ]; then
            READ_ONLY_SWITCH=$(( READ_ONLY_PERCENT >= PCT_AVAILABLE || READ_ONLY_MB >= MB_AVAILABLE ))
            if [ ${READ_ONLY_SWITCH} -ne 0 ]; then
                log "Switching database to read only to prevent the WAL disk from filling up"
                psql -q --file - <<__SQL__
                    ALTER SYSTEM SET default_transaction_read_only to 'on';
                    SELECT pg_reload_conf();

                    -- By immediately resetting the value, but not reloading the server, we ensure that if the server crashes,
                    -- or the Pod is rescheduled, the server will switch back to the default: read-write
                    ALTER SYSTEM RESET default_transaction_read_only;

                    \x on
                    WITH terminated(session_info, terminate) AS (
                        SELECT
                            json_build_object(
                                'usename', usename,
                                'datname', datname,
                                'pid', pid,
                                'application_name', application_name,
                                'state', state,
                                'query_start', query_start
                            ) AS session_info,
                            pg_terminate_backend(pid)
                        FROM
                            pg_stat_activity
                        WHERE
                            datname IS NOT null
                            AND pid != pg_backend_pid()
                            AND application_name != 'Patroni'
                    )
                    SELECT
                        'Preventing full WAL disk: Terminated sessions' AS message,
                        json_agg(session_info) AS terminated_sessions
                    FROM
                        terminated
                    GROUP BY
                        1;

                    -- A checkpoint might actually solve the out of disk space issue as it may be able to
                    -- remove WAL segments. At least it ensures a fast recovery if this instance is going to
                    -- crash/get an ungraceful shutdown.
                    CHECKPOINT;
    __SQL__
                echo "default_transaction_read_only" > "${WAL_STATUS_FILE}"
            fi
        else
            READ_WRITE_SWITCH=$(( READ_WRITE_PERCENT < PCT_AVAILABLE && READ_WRITE_MB < MB_AVAILABLE ))
            if [ ${READ_WRITE_SWITCH} -ne 0 ]; then
                log "Resuming read write connections as the WAL disk has enough free space"
                psql -q --file - <<__SQL__
                SELECT pg_reload_conf();
        ALTER SYSTEM RESET default_transaction_read_only;
        SELECT pg_reload_conf();
    __SQL__
                echo > "${WAL_STATUS_FILE}"
            fi
        fi
    }

    prevent_full_wal "$1"
    {{- end }}

    PGBACKREST_BACKUP_ENABLED={{ or .Values.backup.enable .Values.backup.enabled | int }}
    [ ${PGBACKREST_BACKUP_ENABLED} -ne 0 ] || exit 0

    . "{{ template "pgbackrest_environment_file" }}"
    exec pgbackrest --stanza=poddb archive-push "$@"
  pgbackrest_archive_get.sh: |
    #!/bin/sh
    PGBACKREST_BACKUP_ENABLED={{ or .Values.backup.enable .Values.backup.enabled | int }}
    [ ${PGBACKREST_BACKUP_ENABLED} -ne 0 ] || exit 0

    . "{{ template "pgbackrest_environment_file" }}"
    exec pgbackrest --stanza=poddb archive-get "${1}" "${2}"
  pgbackrest_bootstrap.sh: |
    #!/bin/sh
    set -e

    log() {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - bootstrap - $1"
    }

    terminate() {
        log "Stopping"
        exit 1
    }
    # If we don't catch these signals, and we're still waiting for PostgreSQL
    # to be ready, we will not respond at all to a regular shutdown request,
    # therefore, we explicitly terminate if we receive these signals.
    trap terminate TERM QUIT

    while ! pg_isready -q; do
        log "Waiting for PostgreSQL to become available"
        sleep 3
    done

    # We'll be lazy; we wait for another while to allow the database to promote
    # to primary if it's the only one running
    sleep 10

    # If we are the primary, we want to create/validate the backup stanza
    if [ "$(psql -c "SELECT pg_is_in_recovery()::text" -AtXq)" = "false" ]; then
        pgbackrest check || {
            log "Creating pgBackrest stanza"
            pgbackrest --stanza=poddb stanza-create --log-level-stderr=info || exit 1
            log "Creating initial backup"
            pgbackrest --type=full backup || exit 1
        }
    fi

    log "Starting pgBackrest api to listen for backup requests"
    exec python3 /scripts/pgbackrest-rest.py --stanza=poddb --loglevel=debug
  pgbackrest_restore.sh: |
    #!/bin/sh
    PGBACKREST_BACKUP_ENABLED={{ or .Values.backup.enable .Values.backup.enabled | int }}
    [ ${PGBACKREST_BACKUP_ENABLED} -ne 0 ] || exit 1

    . "{{ template "pod_environment_file" }}"

    PGDATA={{ include "data_directory" . | quote }}
    WALDIR={{ include "wal_directory" . | quote }}

    # A missing PGDATA points to Patroni removing a botched PGDATA, or manual
    # intervention. In this scenario, we need to recreate the DATA and WALDIRs
    # to keep pgBackRest happy
    [ -d "${PGDATA}" ] || install -o postgres -g postgres -d -m 0700 "${PGDATA}"
    [ -d "${WALDIR}" ] || install -o postgres -g postgres -d -m 0700 "${WALDIR}"

    exec pgbackrest --force --delta --log-level-console=detail restore
  restore_or_initdb.sh: |
    #!/bin/sh

    . "{{ template "pod_environment_file" }}"

    log() {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - restore_or_initdb - $1"
    }

    PGDATA={{ include "data_directory" . | quote }}
    WALDIR={{ include "wal_directory" . | quote }}
    # A missing PGDATA points to Patroni removing a botched PGDATA, or manual
    # intervention. In this scenario, we need to recreate the DATA and WALDIRs
    # to keep pgBackRest happy
    [ -d "${PGDATA}" ] || install -o postgres -g postgres -d -m 0700 "${PGDATA}"
    [ -d "${WALDIR}" ] || install -o postgres -g postgres -d -m 0700 "${WALDIR}"

    if [ "${BOOTSTRAP_FROM_BACKUP}" = "1" ]; then
        log "Attempting restore from backup"
        # we want to override the environment with the environment
        # shellcheck disable=SC2046
        export $(env -i envdir {{ template "pgbackrest_bootstrap_environment_dir" . | quote }} env) > /dev/null
        export PGBACKREST_REPO1_PATH={{ index .Values.bootstrapFromBackup "repo1-path" | quote }}

        if [ -z "${PGBACKREST_REPO1_PATH}" ]; then
            log "Unconfigured repository path"
            cat << "__EOT__"

    TimescaleDB Single Helm Chart error:

    You should configure the bootstrapFromBackup in your Helm Chart section by explicitly setting
    the repo1-path to point to the backups.

    For example, if you want to do a disaster recovery, and you want to reuse
    the backup, you could configure the path as follows:

    ```yaml
    bootstrapFromBackup:
      enabled: true
      repo1-path: {{ (printf "/%s/%s/" .Release.Namespace (include "clusterName" . )) | quote }}
    ```

    For more information, consult the admin guide:
    https://github.com/timescale/timescaledb-kubernetes/blob/master/charts/timescaledb-single/admin-guide.md#bootstrap-from-backup


    __EOT__

            exit 1
        fi

        log "Listing available backup information"
        pgbackrest info
        EXITCODE=$?
        if [ ${EXITCODE} -ne 0 ]; then
            exit $EXITCODE
        fi

        pgbackrest --log-level-console=detail restore
        EXITCODE=$?
        if [ ${EXITCODE} -eq 0 ]; then
            log "pgBackRest restore finished succesfully, starting instance in recovery"
            # We want to ensure we do not overwrite a current backup repository with archives, therefore
            # we block archiving from succeeding until Patroni can takeover
            touch "${PGDATA}/recovery.signal"
            pg_ctl -D "${PGDATA}" start -o '--archive-command=/bin/false'

            while ! pg_isready -q; do
                log "Waiting for PostgreSQL to become available"
                sleep 3
            done

            # It is not trivial to figure out to what point we should restore, pgBackRest
            # should be fetching WAL segments until the WAL is exhausted. We'll ask pgBackRest
            # what the Maximum Wal is that it currently has; as soon as we see that, we can consider
            # the restore to be done
            while true; do
              MAX_BACKUP_WAL="$(pgbackrest info --output=json | python3 -c "import json,sys;obj=json.load(sys.stdin); print(obj[0]['archive'][0]['max']);")"
              log "Testing whether WAL file ${MAX_BACKUP_WAL} has been restored ..."
              [ -f "${PGDATA}/pg_wal/${MAX_BACKUP_WAL}" ] && break
              sleep 30;
            done

            # At this point we know the final WAL archive has been restored, we should be done.
            log "The WAL file ${MAX_BACKUP_WAL} has been successully restored, shutting down instance"
            pg_ctl -D "${PGDATA}" promote
            pg_ctl -D "${PGDATA}" stop -m fast
            log "Handing over control to Patroni ..."
        else
            log "Bootstrap from backup failed"
            exit 1
        fi
    else
        # Patroni attaches --scope and --datadir to the arguments, we need to strip them off as
        # initdb has no business with these parameters
        initdb_args=""
        for value in "$@"
        do
            case $value in
                "--scope"*)
                    ;;
                "--datadir"*)
                    ;;
                *)
                    initdb_args="${initdb_args} $value"
                    ;;
            esac
        done

        log "Invoking initdb"
        # shellcheck disable=SC2086
        initdb --auth-local=peer --auth-host=md5 --pgdata="${PGDATA}" --waldir="${WALDIR}" ${initdb_args}
    fi

    echo "include_if_exists = '{{ template "tstune_config". }}'" >> "${PGDATA}/postgresql.conf"
  post_init.sh: |
    #!/bin/sh
    . "{{ template "pod_environment_file" }}"

    log() {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - post_init - $1"
    }

    log "Creating extension TimescaleDB in template1 and postgres databases"
    psql -d "$URL" <<__SQL__
      \connect template1
      -- As we're still only initializing, we cannot have synchronous_commit enabled just yet.
      SET synchronous_commit to 'off';
      CREATE EXTENSION timescaledb;

      \connect postgres
      SET synchronous_commit to 'off';
      CREATE EXTENSION timescaledb;
    __SQL__

    TABLESPACES={{ $.Values.persistentVolumes.tablespaces | default dict | keys | join " " | quote }}
    for tablespace in $TABLESPACES
    do
      log "Creating tablespace ${tablespace}"
      tablespacedir="{{ include "tablespaces_dir" . }}/${tablespace}/data"
      psql -d "$URL" --set tablespace="${tablespace}" --set directory="${tablespacedir}" --set ON_ERROR_STOP=1 <<__SQL__
        SET synchronous_commit to 'off';
        CREATE TABLESPACE :"tablespace" LOCATION :'directory';
    __SQL__
    done

    # This directory may contain user defined post init steps
    for file in {{ template "post_init_dir" . | quote }}/*
    do
      [ -d "$file" ] && continue
      [ ! -r "$file" ] && continue

      case "$file" in
        *.sh)
          if [ -x "$file" ]; then
            log "Call post init script [ $file ]"
            "$file" "$@"
            EXITCODE=$?
          else
            log "Source post init script [ $file ]"
            . "$file"
            EXITCODE=$?
          fi
          ;;
        *.sql)
          log "Apply post init sql [ $file ]"
          # Disable synchronous_commit since we're initializing
          PGOPTIONS="-c synchronous_commit=local" psql -d "$URL" -f "$file"
          EXITCODE=$?
          ;;
        *.sql.gz)
          log "Decompress and apply post init sql [ $file ]"
          gunzip -c "$file" | PGOPTIONS="-c synchronous_commit=local" psql -d "$URL"
          EXITCODE=$?
          ;;
        *)
          log "Ignore unknown post init file type [ $file ]"
          EXITCODE=0
          ;;
      esac
        EXITCODE=$?
        if [ "$EXITCODE" != "0" ]
        then
            log "ERROR: post init script $file exited with exitcode $EXITCODE"
            exit $EXITCODE
        fi
    done

    # We exit 0 this script, otherwise the database initialization fails.
    exit 0
  patroni_callback.sh: |
    #!/bin/sh
    set -e

    . "{{ template "pod_environment_file" }}"

    for suffix in "$1" all
    do
      CALLBACK="{{ template "callbacks_dir" .}}/${suffix}"
      if [ -f "${CALLBACK}" ]
      then
        "${CALLBACK}" "$@"
      fi
    done
{{/*
  # Doing a checkpoint (at the primary and the current instance) before starting
  # the shutdown process will speed up the CHECKPOINT that is part of the shutdown
  # process and the recovery after the pod is rescheduled.
  #
  # We issue the CHECKPOINT at the primary always because:
  #
  # > Restartpoints can't be performed more frequently than checkpoints in the
  # > master because restartpoints can only be performed at checkpoint records.
  # https://www.postgresql.org/docs/current/wal-configuration.html
  #
  # While we're doing these preStop CHECKPOINTs we can still serve read/write
  # queries to clients, whereas as soon as we initiate the shutdown, we terminate
  # connections.
  #
  # This therefore reduces downtime for the clients, at the cost of increasing (slightly)
  # the time to stop the pod, and reducing write performance on the primary.
  #
  # To further reduce downtime for clients, we will issue a switchover iff we are currently
  # running as the primary. This again should be relatively fast, as we've just issued and
  # waited for the CHECKPOINT to complete.
  #
  # This is quite a lot of logic and work in a preStop command; however, if the preStop command
  # fails for whatever reason, the normal Pod shutdown will commence, so it is only able to
  # improve stuff without being able to break stuff.
  # (The $(hostname) inside the switchover call safeguards that we never accidentally
  # switchover the wrong primary).
*/}}
  lifecycle_preStop.psql: |
    \pset pager off
    \set ON_ERROR_STOP true
    \set hostname `hostname`
    \set dsn_fmt 'user=postgres host=%s application_name=lifecycle:preStop@%s connect_timeout=5 options=''-c log_min_duration_statement=0'''

    SELECT
        pg_is_in_recovery() AS in_recovery,
        format(:'dsn_fmt', patroni_scope,                       :'hostname') AS primary_dsn,
        format(:'dsn_fmt', '{{ template "socket_directory" }}', :'hostname') AS local_dsn
    FROM
        current_setting('cluster_name') AS cs(patroni_scope)
    \gset

    \timing on
    \set ECHO queries

    -- There should be a CHECKPOINT at the primary
    \if :in_recovery
        \connect :"primary_dsn"
        CHECKPOINT;
    \endif

    -- There should also be a CHECKPOINT locally,
    -- for the primary, this may mean we do a double checkpoint,
    -- but the second one would be cheap anyway, so we leave that as is
    \connect :"local_dsn"
    SELECT 'Issuing checkpoint';
    CHECKPOINT;

    \if :in_recovery
        SELECT 'We are a replica: Successfully invoked checkpoints at the primary and locally.';
    \else
        SELECT 'We are a primary: Successfully invoked checkpoints, now issuing a switchover.';
        \! curl -s http://localhost:8008/switchover -XPOST -d '{"leader": "$(hostname)"}'
    \endif
...
