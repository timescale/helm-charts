# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

{{- /*
This ConfigMap contains scripts, like bootstrap scripts for the backup and archive scripts for
archive_command
*/ -}}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ template "timescaledb.fullname" . }}-scripts
  labels:
    app: {{ template "timescaledb.fullname" . }}
    chart: {{ template "timescaledb.chart" . }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
    cluster-name: {{ template "clusterName" . }}
data:
  # If no backup is configured, archive_command would normally fail. A failing archive_command on a cluster
  # is going to cause WAL to be kept around forever, meaning we'll fill up Volumes we have quite quickly.
  #
  # Therefore, if the backup is disabled, we always return exitcode 0 when archiving
  pgbackrest_archive.sh: |
    #!/bin/bash
    PGBACKREST_BACKUP_ENABLED={{ or .Values.backup.enable .Values.backup.enabled | int }}
    [ "${PGBACKREST_BACKUP_ENABLED}" == "0" ] && exit 0

    source "{{ template "pgbackrest_environment_file" }}"
    exec pgbackrest --stanza=poddb archive-push $1
  pgbackrest_archive_get.sh: |
    #!/bin/bash
    PGBACKREST_BACKUP_ENABLED={{ or .Values.backup.enable .Values.backup.enabled | int }}
    [ "${PGBACKREST_BACKUP_ENABLED}" == "0" ] && exit 1

    source "{{ template "pgbackrest_environment_file" }}"
    exec pgbackrest --stanza=poddb archive-get ${1} "${2}"
  pgbackrest_bootstrap.sh: |
    #!/bin/bash
    set -e

    function log {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - bootstrap - $1"
    }

    function terminate() {
        log "Stopping"
        exit 1
    }
    # If we don't catch these signals, and we're still waiting for PostgreSQL
    # to be ready, we will not respond at all to a regular shutdown request,
    # therefore, we explicitly terminate if we receive these signals.
    trap terminate SIGTERM SIGQUIT

    while ! pg_isready -q; do
        log "Waiting for PostgreSQL to become available"
        sleep 3
    done

    # We'll be lazy; we wait for another while to allow the database to promote
    # to primary if it's the only one running
    sleep 10

    # If we are the primary, we want to create/validate the backup stanza
    if [ "$(psql -c "SELECT pg_is_in_recovery()::text" -AtXq)" == "false" ]; then
        pgbackrest check || {
            log "Creating pgBackrest stanza"
            pgbackrest --stanza=poddb stanza-create --log-level-stderr=info || exit 1
            log "Creating initial backup"
            pgbackrest --type=full backup || exit 1
        }
    fi

    log "Starting pgBackrest api to listen for backup requests"
    exec python3 /scripts/pgbackrest-rest.py --stanza=poddb --loglevel=debug
  pgbackrest_restore.sh: |
    #!/bin/bash
    PGBACKREST_BACKUP_ENABLED={{ or .Values.backup.enable .Values.backup.enabled | int }}
    [ "${PGBACKREST_BACKUP_ENABLED}" == "0" ] && exit 1

    source "{{ template "pod_environment_file" }}"

    PGDATA={{ include "data_directory" . | quote }}
    WALDIR={{ include "wal_directory" . | quote }}

    # A missing PGDATA points to Patroni removing a botched PGDATA, or manual
    # intervention. In this scenario, we need to recreate the DATA and WALDIRs
    # to keep pgBackRest happy
    [ -d "${PGDATA}" ] || install -o postgres -g postgres -d -m 0700 "${PGDATA}"
    [ -d "${WALDIR}" ] || install -o postgres -g postgres -d -m 0700 "${WALDIR}"

    exec pgbackrest --force --delta --log-level-console=detail restore
  restore_or_initdb.sh: |
    #!/bin/bash

    source "{{ template "pod_environment_file" }}"

    function log {
      echo "$(date '+%Y-%m-%d %H:%M:%S') - restore_or_initdb - $1"
    }

    PGDATA={{ include "data_directory" . | quote }}
    WALDIR={{ include "wal_directory" . | quote }}

    # Patroni attaches --scope and --datadir to the arguments, we need to strip them off as
    # initdb has no business with these parameters
    initdb_args=""
    for value in "$@"
    do
      [[ $value == --scope* ]] || [[ $value == --datadir* ]] || initdb_args="${initdb_args} $value"
    done

    log "Invoking initdb"
    initdb --auth-local=peer --auth-host=md5 --pgdata="${PGDATA}" --waldir="${WALDIR}" ${initdb_args}
    echo "include_if_exists = '{{ template "tstune_config". }}'" >> "${PGDATA}/postgresql.conf"

  post_init.sh: |
    #!/bin/bash
    source "{{ template "pod_environment_file" }}"

    function log {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - post_init - $1"
    }

    log "Creating extension TimescaleDB in template1 and postgres databases"
    psql -d "$URL" <<__SQL__
      \connect template1
      -- As we're still only initializing, we cannot have synchronous_commit enabled just yet.
      SET synchronous_commit to 'off';
      CREATE EXTENSION timescaledb;

      \connect postgres
      SET synchronous_commit to 'off';
      CREATE EXTENSION timescaledb;
    __SQL__

    TABLESPACES={{ $.Values.persistentVolumes.tablespaces | default dict | keys | join " " | quote }}
    for tablespace in $TABLESPACES
    do
      log "Creating tablespace ${tablespace}"
      tablespacedir="{{ include "tablespaces_dir" . }}/${tablespace}/data"
      psql -d "$URL" --set tablespace="${tablespace}" --set directory="${tablespacedir}" --set ON_ERROR_STOP=1 <<__SQL__
        SET synchronous_commit to 'off';
        CREATE TABLESPACE :"tablespace" LOCATION :'directory';
    __SQL__
    done

    # We always exit 0 this script, otherwise the database initialization fails.
    exit 0
  patroni_callback.sh: |
    #!/bin/bash
    set -e

    source "{{ template "pod_environment_file" }}"

    for suffix in "$1" all
    do
      CALLBACK="{{ template "callbacks_dir" .}}/${suffix}"
      if [ -f "${CALLBACK}" ]
      then
        "${CALLBACK}" $@
      fi
    done
{{/*
  # Doing a checkpoint (at the primary and the current instance) before starting
  # the shutdown process will speed up the CHECKPOINT that is part of the shutdown
  # process and the recovery after the pod is rescheduled.
  #
  # We issue the CHECKPOINT at the primary always because:
  #
  # > Restartpoints can't be performed more frequently than checkpoints in the
  # > master because restartpoints can only be performed at checkpoint records.
  # https://www.postgresql.org/docs/current/wal-configuration.html
  #
  # While we're doing these preStop CHECKPOINTs we can still serve read/write
  # queries to clients, whereas as soon as we initiate the shutdown, we terminate
  # connections.
  #
  # This therefore reduces downtime for the clients, at the cost of increasing (slightly)
  # the time to stop the pod, and reducing write performance on the primary.
  #
  # To further reduce downtime for clients, we will issue a switchover iff we are currently
  # running as the primary. This again should be relatively fast, as we've just issued and
  # waited for the CHECKPOINT to complete.
  #
  # This is quite a lot of logic and work in a preStop command; however, if the preStop command
  # fails for whatever reason, the normal Pod shutdown will commence, so it is only able to
  # improve stuff without being able to break stuff.
  # (The $(hostname) inside the switchover call safeguards that we never accidentally
  # switchover the wrong primary).
*/}}
  lifecycle_preStop.psql: |
    \pset pager off
    \set ON_ERROR_STOP true
    \set hostname `hostname`
    \set dsn_fmt 'user=postgres host=%s application_name=lifecycle:preStop@%s connect_timeout=5 options=''-c log_min_duration_statement=0'''

    SELECT
        pg_is_in_recovery() AS in_recovery,
        format(:'dsn_fmt', patroni_scope,                       :'hostname') AS primary_dsn,
        format(:'dsn_fmt', '{{ template "socket_directory" }}', :'hostname') AS local_dsn
    FROM
        current_setting('cluster_name') AS cs(patroni_scope)
    \gset

    \timing on
    \set ECHO queries

    -- There should be a CHECKPOINT at the primary
    \if :in_recovery
        \connect :"primary_dsn"
        CHECKPOINT;
    \endif

    -- There should also be a CHECKPOINT locally,
    -- for the primary, this may mean we do a double checkpoint,
    -- but the second one would be cheap anyway, so we leave that as is
    \connect :"local_dsn"
    SELECT 'Issuing checkpoint';
    CHECKPOINT;

    \if :in_recovery
        SELECT 'We are a replica: Successfully invoked checkpoints at the primary and locally.';
    \else
        SELECT 'We are a primary: Successfully invoked checkpoints, now issuing a switchover.';
        \! curl -s http://localhost:8008/switchover -XPOST -d '{"leader": "$(hostname)"}'
    \endif
...
